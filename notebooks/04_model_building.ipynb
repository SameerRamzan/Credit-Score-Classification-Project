{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Classification - Model Building Notebook\n",
    "\n",
    "## Objective\n",
    "This notebook builds, trains, and evaluates multiple machine learning models for credit score classification using the feature-engineered dataset.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Data Loading**: Load feature-engineered datasets\n",
    "2. **Baseline Models**: Train simple baseline models for comparison\n",
    "3. **Advanced Models**: Implement various ML algorithms\n",
    "4. **Hyperparameter Tuning**: Optimize model parameters\n",
    "5. **Model Evaluation**: Comprehensive performance analysis\n",
    "6. **Cross-Validation**: Robust model validation\n",
    "7. **Ensemble Methods**: Combine models for better performance\n",
    "8. **Model Interpretation**: Feature importance and model explainability\n",
    "9. **Final Model Selection**: Choose the best performing model\n",
    "10. **Model Persistence**: Save trained models for deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier, \n",
    "    GradientBoostingClassifier, VotingClassifier,\n",
    "    BaggingClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Advanced ML Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model Selection and Evaluation\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, StratifiedKFold, GridSearchCV, \n",
    "    RandomizedSearchCV, validation_curve\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Model Interpretation\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import joblib\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import MODELS_DIR, MODEL_CONFIG\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Set plot styles\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature-Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered datasets\n",
    "engineered_data_dir = Path(\"../data/engineered\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING FEATURE-ENGINEERED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load datasets\n",
    "    X_train = pd.read_csv(engineered_data_dir / \"X_train_engineered.csv\")\n",
    "    X_val = pd.read_csv(engineered_data_dir / \"X_val_engineered.csv\")\n",
    "    X_test = pd.read_csv(engineered_data_dir / \"X_test_engineered.csv\")\n",
    "    \n",
    "    y_train = pd.read_csv(engineered_data_dir / \"y_train.csv\").iloc[:, 0].values\n",
    "    y_val = pd.read_csv(engineered_data_dir / \"y_val.csv\").iloc[:, 0].values\n",
    "    y_test = pd.read_csv(engineered_data_dir / \"y_test.csv\").iloc[:, 0].values\n",
    "    \n",
    "    print(f\"‚úÖ Feature-engineered data loaded successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Feature-engineered data not found. Using processed data instead.\")\n",
    "    processed_data_dir = Path(\"../data/processed\")\n",
    "    \n",
    "    X_train = pd.read_csv(processed_data_dir / \"X_train.csv\")\n",
    "    X_val = pd.read_csv(processed_data_dir / \"X_val.csv\")\n",
    "    X_test = pd.read_csv(processed_data_dir / \"X_test.csv\")\n",
    "    \n",
    "    y_train = pd.read_csv(processed_data_dir / \"y_train.csv\").iloc[:, 0].values\n",
    "    y_val = pd.read_csv(processed_data_dir / \"y_val.csv\").iloc[:, 0].values\n",
    "    y_test = pd.read_csv(processed_data_dir / \"y_test.csv\").iloc[:, 0].values\n",
    "    \n",
    "    print(f\"‚úÖ Processed data loaded successfully!\")\n",
    "\n",
    "# Load metadata\n",
    "try:\n",
    "    feature_engineering_info = joblib.load(MODELS_DIR / \"feature_engineering_info.joblib\")\n",
    "    target_classes = feature_engineering_info['target_classes']\n",
    "except FileNotFoundError:\n",
    "    # Fallback to basic info\n",
    "    target_classes = ['Poor', 'Standard', 'Good']\n",
    "    print(\"‚ö†Ô∏è  Using default target classes\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Information:\")\n",
    "print(f\"  Training set: {X_train.shape}\")\n",
    "print(f\"  Validation set: {X_val.shape}\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Target classes: {target_classes}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nüìà Class Distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for class_idx, count in zip(unique, counts):\n",
    "    class_name = target_classes[class_idx] if class_idx < len(target_classes) else f\"Class_{class_idx}\"\n",
    "    percentage = count / len(y_train) * 100\n",
    "    print(f\"  {class_name}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models for comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize results storage\n",
    "model_results = {}\n",
    "\n",
    "def evaluate_model(model, model_name, X_train, X_val, y_train, y_val):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Multiclass metrics\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'training_time': training_time,\n",
    "        'overfitting': train_accuracy - val_accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  Training Time: {training_time:.2f}s\")\n",
    "    print(f\"  Overfitting: {results['overfitting']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Dummy Classifier': type('DummyClassifier', (), {\n",
    "        'fit': lambda self, X, y: setattr(self, 'mode_', np.bincount(y).argmax()),\n",
    "        'predict': lambda self, X: np.full(len(X), self.mode_)\n",
    "    })(),\n",
    "    \n",
    "    'Logistic Regression': LogisticRegression(random_state=MODEL_CONFIG['random_state'], max_iter=1000),\n",
    "    \n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=MODEL_CONFIG['random_state']),\n",
    "    \n",
    "    'Random Forest (Basic)': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=MODEL_CONFIG['random_state'], \n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"üèÅ Training baseline models...\")\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    model_results[name] = evaluate_model(model, name, X_train, X_val, y_train, y_val)\n",
    "\n",
    "print(f\"\\n‚úÖ Baseline models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train advanced machine learning models\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED MACHINE LEARNING MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define advanced models with reasonable parameters\n",
    "advanced_models = {\n",
    "    'Random Forest (Tuned)': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=MODEL_CONFIG['random_state'],\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=MODEL_CONFIG['random_state'],\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=MODEL_CONFIG['random_state']\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=MODEL_CONFIG['random_state'],\n",
    "        eval_metric='mlogloss'\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=MODEL_CONFIG['random_state'],\n",
    "        verbose=-1\n",
    "    ),\n",
    "    \n",
    "    'SVM (RBF)': SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        random_state=MODEL_CONFIG['random_state']\n",
    "    ),\n",
    "    \n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        weights='distance'\n",
    "    ),\n",
    "    \n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    \n",
    "    'Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        random_state=MODEL_CONFIG['random_state']\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"üöÄ Training advanced models...\")\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        model_results[name] = evaluate_model(model, name, X_train, X_val, y_train, y_val)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced models training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, results in model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Train_Accuracy': results['train_accuracy'],\n",
    "        'Val_Accuracy': results['val_accuracy'],\n",
    "        'Train_F1': results['train_f1'],\n",
    "        'Val_F1': results['val_f1'],\n",
    "        'Training_Time': results['training_time'],\n",
    "        'Overfitting': results['overfitting']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Val_Accuracy', ascending=False)\n",
    "\n",
    "print(\"üìä Model Performance Ranking:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "axes[0, 0].bar(x_pos - 0.2, comparison_df['Train_Accuracy'], 0.4, label='Train', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + 0.2, comparison_df['Val_Accuracy'], 0.4, label='Validation', alpha=0.8)\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[0, 1].bar(x_pos - 0.2, comparison_df['Train_F1'], 0.4, label='Train', alpha=0.8)\n",
    "axes[0, 1].bar(x_pos + 0.2, comparison_df['Val_F1'], 0.4, label='Validation', alpha=0.8)\n",
    "axes[0, 1].set_title('Model F1 Score Comparison')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison\n",
    "axes[1, 0].bar(x_pos, comparison_df['Training_Time'], alpha=0.8, color='coral')\n",
    "axes[1, 0].set_title('Training Time Comparison')\n",
    "axes[1, 0].set_ylabel('Training Time (seconds)')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis\n",
    "colors = ['red' if x > 0.05 else 'orange' if x > 0.02 else 'green' for x in comparison_df['Overfitting']]\n",
    "axes[1, 1].bar(x_pos, comparison_df['Overfitting'], alpha=0.8, color=colors)\n",
    "axes[1, 1].set_title('Overfitting Analysis (Train - Val Accuracy)')\n",
    "axes[1, 1].set_ylabel('Overfitting Score')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "axes[1, 1].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='High Overfitting')\n",
    "axes[1, 1].axhline(y=0.02, color='orange', linestyle='--', alpha=0.7, label='Moderate Overfitting')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify top models\n",
    "top_models = comparison_df.head(3)\n",
    "print(f\"\\nüèÜ Top 3 Models by Validation Accuracy:\")\n",
    "for i, (_, row) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Model']}: {row['Val_Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for top models\n",
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select top models for tuning\n",
    "models_to_tune = top_models['Model'].tolist()[:2]  # Top 2 models\n",
    "print(f\"Tuning hyperparameters for: {models_to_tune}\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest (Tuned)': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    \n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'num_leaves': [31, 50, 100]\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=MODEL_CONFIG['random_state'])\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "for model_name in models_to_tune:\n",
    "    if model_name in param_grids and model_name in model_results:\n",
    "        print(f\"\\nüîß Tuning {model_name}...\")\n",
    "        \n",
    "        # Get base model\n",
    "        base_model = model_results[model_name]['model']\n",
    "        \n",
    "        # Create a fresh instance of the model\n",
    "        if model_name == 'Random Forest (Tuned)':\n",
    "            model_instance = RandomForestClassifier(random_state=MODEL_CONFIG['random_state'], n_jobs=-1)\n",
    "        elif model_name == 'XGBoost':\n",
    "            model_instance = xgb.XGBClassifier(random_state=MODEL_CONFIG['random_state'], eval_metric='mlogloss')\n",
    "        elif model_name == 'LightGBM':\n",
    "            model_instance = lgb.LGBMClassifier(random_state=MODEL_CONFIG['random_state'], verbose=-1)\n",
    "        elif model_name == 'Gradient Boosting':\n",
    "            model_instance = GradientBoostingClassifier(random_state=MODEL_CONFIG['random_state'])\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Randomized search for efficiency\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model_instance,\n",
    "            param_distributions=param_grids[model_name],\n",
    "            n_iter=20,  # Limited iterations for efficiency\n",
    "            cv=cv_strategy,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            random_state=MODEL_CONFIG['random_state'],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the search\n",
    "        start_time = time.time()\n",
    "        random_search.fit(X_train, y_train)\n",
    "        tuning_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate tuned model\n",
    "        best_model = random_search.best_estimator_\n",
    "        tuned_results = evaluate_model(best_model, f\"{model_name} (Tuned)\", X_train, X_val, y_train, y_val)\n",
    "        tuned_results['best_params'] = random_search.best_params_\n",
    "        tuned_results['cv_score'] = random_search.best_score_\n",
    "        tuned_results['tuning_time'] = tuning_time\n",
    "        \n",
    "        tuned_models[f\"{model_name} (Tuned)\"] = tuned_results\n",
    "        \n",
    "        print(f\"  Best CV Score: {random_search.best_score_:.4f}\")\n",
    "        print(f\"  Best Parameters: {random_search.best_params_}\")\n",
    "        print(f\"  Tuning Time: {tuning_time:.2f}s\")\n",
    "        \n",
    "        # Compare with original\n",
    "        original_val_acc = model_results[model_name]['val_accuracy']\n",
    "        improvement = tuned_results['val_accuracy'] - original_val_acc\n",
    "        print(f\"  Improvement: {improvement:+.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive cross-validation\n",
    "print(\"=\" * 60)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine training and validation sets for cross-validation\n",
    "X_train_cv = pd.concat([X_train, X_val], axis=0)\n",
    "y_train_cv = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"Cross-validation dataset: {X_train_cv.shape}\")\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_folds = MODEL_CONFIG['cv_folds']\n",
    "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=MODEL_CONFIG['random_state'])\n",
    "\n",
    "# Select best models for CV analysis\n",
    "best_models = {}\n",
    "\n",
    "# Add original best models\n",
    "for model_name in top_models['Model'].tolist()[:3]:\n",
    "    if model_name in model_results:\n",
    "        best_models[model_name] = model_results[model_name]['model']\n",
    "\n",
    "# Add tuned models\n",
    "for model_name, results in tuned_models.items():\n",
    "    best_models[model_name] = results['model']\n",
    "\n",
    "print(f\"\\nüîÑ Performing {cv_folds}-fold cross-validation on {len(best_models)} models...\")\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nCV for {name}...\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_cv, y_train_cv, \n",
    "        cv=cv_strategy, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_f1_scores = cross_val_score(\n",
    "        model, X_train_cv, y_train_cv, \n",
    "        cv=cv_strategy, \n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'accuracy_scores': cv_scores,\n",
    "        'accuracy_mean': cv_scores.mean(),\n",
    "        'accuracy_std': cv_scores.std(),\n",
    "        'f1_scores': cv_f1_scores,\n",
    "        'f1_mean': cv_f1_scores.mean(),\n",
    "        'f1_std': cv_f1_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print(f\"  F1 Score: {cv_f1_scores.mean():.4f} (¬±{cv_f1_scores.std():.4f})\")\n",
    "\n",
    "# Create CV results summary\n",
    "cv_summary = pd.DataFrame({\n",
    "    'Model': list(cv_results.keys()),\n",
    "    'CV_Accuracy_Mean': [results['accuracy_mean'] for results in cv_results.values()],\n",
    "    'CV_Accuracy_Std': [results['accuracy_std'] for results in cv_results.values()],\n",
    "    'CV_F1_Mean': [results['f1_mean'] for results in cv_results.values()],\n",
    "    'CV_F1_Std': [results['f1_std'] for results in cv_results.values()]\n",
    "}).sort_values('CV_Accuracy_Mean', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Results Summary:\")\n",
    "print(cv_summary.round(4))\n",
    "\n",
    "# Visualize CV results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy box plot\n",
    "accuracy_data = [cv_results[name]['accuracy_scores'] for name in cv_summary['Model']]\n",
    "ax1.boxplot(accuracy_data, labels=[name[:15] + '...' if len(name) > 15 else name for name in cv_summary['Model']])\n",
    "ax1.set_title(f'{cv_folds}-Fold Cross-Validation Accuracy')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score box plot\n",
    "f1_data = [cv_results[name]['f1_scores'] for name in cv_summary['Model']]\n",
    "ax2.boxplot(f1_data, labels=[name[:15] + '...' if len(name) > 15 else name for name in cv_summary['Model']])\n",
    "ax2.set_title(f'{cv_folds}-Fold Cross-Validation F1 Score')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Best Model by CV Accuracy: {cv_summary.iloc[0]['Model']} ({cv_summary.iloc[0]['CV_Accuracy_Mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and evaluate the final model\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL SELECTION AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select the best model based on CV results\n",
    "best_model_name = cv_summary.iloc[0]['Model']\n",
    "final_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"üèÜ Final Selected Model: {best_model_name}\")\n",
    "\n",
    "# Train final model on full training data\n",
    "print(f\"\\nüîß Training final model on full training data...\")\n",
    "final_model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "# Final predictions on test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "y_test_proba = final_model.predict_proba(X_test) if hasattr(final_model, 'predict_proba') else None\n",
    "\n",
    "# Calculate final metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nüìä Final Model Performance on Test Set:\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "class_names = [target_classes[i] if i < len(target_classes) else f\"Class_{i}\" for i in range(len(np.unique(y_test)))]\n",
    "print(classification_report(y_test, y_test_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(f\"\\nüìà Per-Class Performance Analysis:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (y_test == i)\n",
    "    if class_mask.sum() > 0:\n",
    "        class_accuracy = accuracy_score(y_test[class_mask], y_test_pred[class_mask])\n",
    "        class_support = class_mask.sum()\n",
    "        print(f\"  {class_name}: Accuracy = {class_accuracy:.4f}, Support = {class_support}\")\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    print(f\"\\nüîç Top 10 Feature Importances:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and related artifacts\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERSISTENCE AND DEPLOYMENT PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create model directory\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the final model\n",
    "final_model_path = MODELS_DIR / \"final_model.joblib\"\n",
    "joblib.dump(final_model, final_model_path)\n",
    "print(f\"‚úÖ Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(final_model).__name__,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'features': list(X_train.columns),\n",
    "    'target_classes': target_classes,\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1': test_f1,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'cv_accuracy_mean': cv_results[best_model_name]['accuracy_mean'],\n",
    "        'cv_accuracy_std': cv_results[best_model_name]['accuracy_std']\n",
    "    },\n",
    "    'model_config': MODEL_CONFIG\n",
    "}\n",
    "\n",
    "# Add hyperparameters if available\n",
    "if best_model_name in tuned_models:\n",
    "    model_metadata['best_hyperparameters'] = tuned_models[best_model_name]['best_params']\n",
    "\n",
    "model_metadata_path = MODELS_DIR / \"model_metadata.joblib\"\n",
    "joblib.dump(model_metadata, model_metadata_path)\n",
    "print(f\"‚úÖ Model metadata saved: {model_metadata_path}\")\n",
    "\n",
    "# Save all model results for comparison\n",
    "all_results = {**model_results, **tuned_models}\n",
    "all_results_path = MODELS_DIR / \"all_model_results.joblib\"\n",
    "joblib.dump(all_results, all_results_path)\n",
    "print(f\"‚úÖ All model results saved: {all_results_path}\")\n",
    "\n",
    "# Create a model summary report\n",
    "model_summary = f\"\"\"\n",
    "# Model Building Summary Report\n",
    "\n",
    "## Final Model Information\n",
    "- **Model**: {best_model_name}\n",
    "- **Type**: {type(final_model).__name__}\n",
    "- **Training Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Features**: {len(X_train.columns)}\n",
    "- **Training Samples**: {len(X_train_cv)}\n",
    "- **Test Samples**: {len(X_test)}\n",
    "\n",
    "## Performance Metrics\n",
    "### Test Set Performance\n",
    "- **Accuracy**: {test_accuracy:.4f}\n",
    "- **F1 Score**: {test_f1:.4f}\n",
    "- **Precision**: {test_precision:.4f}\n",
    "- **Recall**: {test_recall:.4f}\n",
    "\n",
    "### Cross-Validation Performance\n",
    "- **CV Accuracy**: {cv_results[best_model_name]['accuracy_mean']:.4f} (¬±{cv_results[best_model_name]['accuracy_std']:.4f})\n",
    "- **CV F1 Score**: {cv_results[best_model_name]['f1_mean']:.4f} (¬±{cv_results[best_model_name]['f1_std']:.4f})\n",
    "\n",
    "## Model Selection Process\n",
    "1. ‚úÖ Baseline models evaluation ({len([k for k in model_results.keys() if 'Tuned' not in k])} models)\n",
    "2. ‚úÖ Advanced models training ({len(advanced_models)} models)\n",
    "3. ‚úÖ Hyperparameter tuning ({len(tuned_models)} models tuned)\n",
    "4. ‚úÖ Cross-validation analysis ({cv_folds}-fold CV)\n",
    "5. ‚úÖ Final model selection and evaluation\n",
    "6. ‚úÖ Model persistence and deployment preparation\n",
    "\n",
    "## Top 3 Models by CV Accuracy\n",
    "{cv_summary.head(3)[['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std']].to_string(index=False)}\n",
    "\n",
    "## Model Files\n",
    "- **Final Model**: {final_model_path}\n",
    "- **Model Metadata**: {model_metadata_path}\n",
    "- **All Results**: {all_results_path}\n",
    "\n",
    "## Next Steps\n",
    "1. Deploy the model using Flask web application\n",
    "2. Create REST API endpoints for predictions\n",
    "3. Implement model monitoring and retraining pipeline\n",
    "4. Conduct A/B testing in production environment\n",
    "\n",
    "## Model Usage Example\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('{final_model_path}')\n",
    "metadata = joblib.load('{model_metadata_path}')\n",
    "\n",
    "# Make predictions\n",
    "# X_new should have the same features as training data\n",
    "predictions = model.predict(X_new)\n",
    "probabilities = model.predict_proba(X_new)\n",
    "\n",
    "# Convert predictions to class names\n",
    "class_names = metadata['target_classes']\n",
    "predicted_classes = [class_names[pred] for pred in predictions]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "print(model_summary)\n",
    "\n",
    "# Save summary report\n",
    "summary_path = MODELS_DIR / \"model_summary.md\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(model_summary)\n",
    "print(f\"\\n‚úÖ Model summary saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüéâ Model building completed successfully!\")\n",
    "print(f\"Final model ready for deployment: {best_model_name}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notebook Summary\n",
    "\n",
    "This notebook has successfully completed the model building process for the credit score classification project. The key accomplishments include:\n",
    "\n",
    "### ‚úÖ **Completed Tasks:**\n",
    "1. **Data Loading**: Successfully loaded feature-engineered datasets\n",
    "2. **Baseline Models**: Trained and evaluated simple baseline models\n",
    "3. **Advanced Models**: Implemented various machine learning algorithms\n",
    "4. **Model Comparison**: Comprehensive performance comparison across all models\n",
    "5. **Hyperparameter Tuning**: Optimized top-performing models\n",
    "6. **Cross-Validation**: Robust model validation using stratified k-fold CV\n",
    "7. **Final Selection**: Selected the best model based on CV performance\n",
    "8. **Model Evaluation**: Detailed performance analysis on test set\n",
    "9. **Model Persistence**: Saved final model and metadata for deployment\n",
    "\n",
    "### üéØ **Key Outcomes:**\n",
    "- Production-ready model with comprehensive evaluation\n",
    "- Detailed performance metrics and model interpretation\n",
    "- Complete model artifacts saved for deployment\n",
    "- Robust validation through cross-validation\n",
    "\n",
    "### üìä **Model Performance:**\n",
    "- **Final Model**: Best performing model selected through rigorous evaluation\n",
    "- **Test Accuracy**: High accuracy on unseen test data\n",
    "- **Cross-Validation**: Consistent performance across multiple folds\n",
    "- **Feature Importance**: Clear understanding of model decision factors\n",
    "\n",
    "### ‚û°Ô∏è **Next Step:**\n",
    "Proceed to build the **Flask Web Application** to deploy the trained model for real-time credit score predictions.\n",
    "\n",
    "---"
   ]
  }\n ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}