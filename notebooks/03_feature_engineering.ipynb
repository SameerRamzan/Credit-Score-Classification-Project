{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Classification - Feature Engineering Notebook\n",
    "\n",
    "## Objective\n",
    "This notebook performs advanced feature engineering on the preprocessed credit score dataset to create meaningful features that can improve model performance.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Data Loading**: Load preprocessed datasets and encoders\n",
    "2. **Domain Knowledge Features**: Create features based on financial domain expertise\n",
    "3. **Statistical Features**: Generate statistical aggregations and ratios\n",
    "4. **Interaction Features**: Create feature interactions and combinations\n",
    "5. **Polynomial Features**: Generate polynomial and transformation features\n",
    "6. **Feature Selection**: Select the most important features using various techniques\n",
    "7. **Feature Validation**: Validate new features for usefulness\n",
    "8. **Dimensionality Analysis**: Analyze the final feature space\n",
    "9. **Export Enhanced Dataset**: Save the feature-engineered dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Feature engineering and selection\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, mutual_info_classif,\n",
    "    RFE, RFECV, SelectFromModel\n",
    ")\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import MODELS_DIR, FEATURE_CONFIG\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plot styles\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed datasets\n",
    "processed_data_dir = Path(\"../data/processed\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING PREPROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load datasets\n",
    "X_train = pd.read_csv(processed_data_dir / \"X_train.csv\")\n",
    "X_val = pd.read_csv(processed_data_dir / \"X_val.csv\")\n",
    "X_test = pd.read_csv(processed_data_dir / \"X_test.csv\")\n",
    "\n",
    "y_train = pd.read_csv(processed_data_dir / \"y_train.csv\").iloc[:, 0].values\n",
    "y_val = pd.read_csv(processed_data_dir / \"y_val.csv\").iloc[:, 0].values\n",
    "y_test = pd.read_csv(processed_data_dir / \"y_test.csv\").iloc[:, 0].values\n",
    "\n",
    "# Load encoders and feature info\n",
    "encoders = joblib.load(MODELS_DIR / \"preprocessors.joblib\")\n",
    "feature_info = joblib.load(MODELS_DIR / \"feature_info.joblib\")\n",
    "\n",
    "print(f\"📊 Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Feature count: {len(feature_info['feature_names'])}\")\n",
    "print(f\"Target classes: {feature_info['target_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine current features\n",
    "print(\"\\n📋 Current Features:\")\n",
    "for i, feature in enumerate(feature_info['feature_names'], 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Memory usage: {X_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Domain Knowledge Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create domain-specific features based on financial expertise\n",
    "print(\"=\" * 60)\n",
    "print(\"DOMAIN KNOWLEDGE FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_domain_features(df):\n",
    "    \"\"\"\n",
    "    Create domain-specific features for credit score prediction\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Financial Health Indicators\n",
    "    print(\"🏦 Creating Financial Health Indicators...\")\n",
    "    \n",
    "    # 1. Debt-to-Income Ratio\n",
    "    if 'Outstanding_Debt' in df_new.columns and 'Annual_Income' in df_new.columns:\n",
    "        df_new['Debt_to_Income_Ratio'] = np.where(\n",
    "            df_new['Annual_Income'] > 0,\n",
    "            df_new['Outstanding_Debt'] / df_new['Annual_Income'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # 2. Monthly Debt Service Ratio\n",
    "    if 'Total_EMI_per_month' in df_new.columns and 'Monthly_Inhand_Salary' in df_new.columns:\n",
    "        df_new['Monthly_Debt_Service_Ratio'] = np.where(\n",
    "            df_new['Monthly_Inhand_Salary'] > 0,\n",
    "            df_new['Total_EMI_per_month'] / df_new['Monthly_Inhand_Salary'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # 3. Savings Rate\n",
    "    if 'Amount_invested_monthly' in df_new.columns and 'Monthly_Inhand_Salary' in df_new.columns:\n",
    "        df_new['Savings_Rate'] = np.where(\n",
    "            df_new['Monthly_Inhand_Salary'] > 0,\n",
    "            df_new['Amount_invested_monthly'] / df_new['Monthly_Inhand_Salary'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # 4. Available Credit Capacity\n",
    "    if 'Monthly_Balance' in df_new.columns and 'Monthly_Inhand_Salary' in df_new.columns:\n",
    "        df_new['Available_Credit_Capacity'] = np.where(\n",
    "            df_new['Monthly_Inhand_Salary'] > 0,\n",
    "            df_new['Monthly_Balance'] / df_new['Monthly_Inhand_Salary'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # Credit Behavior Indicators\n",
    "    print(\"💳 Creating Credit Behavior Indicators...\")\n",
    "    \n",
    "    # 5. Credit Account Diversity Score\n",
    "    if 'Num_Bank_Accounts' in df_new.columns and 'Num_Credit_Card' in df_new.columns:\n",
    "        df_new['Credit_Account_Diversity'] = (\n",
    "            df_new['Num_Bank_Accounts'] + df_new['Num_Credit_Card']\n",
    "        )\n",
    "    \n",
    "    # 6. Credit Experience Score\n",
    "    if 'Credit_History_Age' in df_new.columns and 'Age' in df_new.columns:\n",
    "        df_new['Credit_Experience_Ratio'] = np.where(\n",
    "            df_new['Age'] > 0,\n",
    "            df_new['Credit_History_Age'] / (df_new['Age'] * 12),  # Convert age to months\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # 7. Payment Reliability Score\n",
    "    if 'Num_of_Delayed_Payment' in df_new.columns and 'Delay_from_due_date' in df_new.columns:\n",
    "        df_new['Payment_Reliability_Score'] = (\n",
    "            1 / (1 + df_new['Num_of_Delayed_Payment'] + df_new['Delay_from_due_date'] / 30)\n",
    "        )\n",
    "    \n",
    "    # Risk Assessment Features\n",
    "    print(\"⚠️  Creating Risk Assessment Features...\")\n",
    "    \n",
    "    # 8. Financial Stress Indicator\n",
    "    if 'Outstanding_Debt' in df_new.columns and 'Monthly_Inhand_Salary' in df_new.columns:\n",
    "        monthly_debt_burden = np.where(\n",
    "            df_new['Monthly_Inhand_Salary'] > 0,\n",
    "            df_new['Outstanding_Debt'] / (df_new['Monthly_Inhand_Salary'] * 12),\n",
    "            0\n",
    "        )\n",
    "        df_new['Financial_Stress_Score'] = np.where(\n",
    "            monthly_debt_burden > 0.5, 1,  # High stress\n",
    "            np.where(monthly_debt_burden > 0.3, 0.5, 0)  # Medium/Low stress\n",
    "        )\n",
    "    \n",
    "    # 9. Income Stability Indicator\n",
    "    if 'Annual_Income' in df_new.columns and 'Monthly_Inhand_Salary' in df_new.columns:\n",
    "        expected_monthly = df_new['Annual_Income'] / 12\n",
    "        df_new['Income_Stability'] = np.where(\n",
    "            expected_monthly > 0,\n",
    "            1 - abs(df_new['Monthly_Inhand_Salary'] - expected_monthly) / expected_monthly,\n",
    "            0\n",
    "        )\n",
    "        df_new['Income_Stability'] = np.clip(df_new['Income_Stability'], 0, 1)\n",
    "    \n",
    "    # Behavioral Features\n",
    "    print(\"🎯 Creating Behavioral Features...\")\n",
    "    \n",
    "    # 10. Credit Utilization Efficiency\n",
    "    if 'Credit_Utilization_Ratio' in df_new.columns:\n",
    "        # Optimal utilization is around 10-30%\n",
    "        df_new['Credit_Utilization_Efficiency'] = np.where(\n",
    "            (df_new['Credit_Utilization_Ratio'] >= 10) & (df_new['Credit_Utilization_Ratio'] <= 30),\n",
    "            1,  # Optimal range\n",
    "            np.where(\n",
    "                df_new['Credit_Utilization_Ratio'] < 10,\n",
    "                0.8,  # Under-utilization\n",
    "                1 - (df_new['Credit_Utilization_Ratio'] - 30) / 70  # Over-utilization penalty\n",
    "            )\n",
    "        )\n",
    "        df_new['Credit_Utilization_Efficiency'] = np.clip(df_new['Credit_Utilization_Efficiency'], 0, 1)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply domain feature engineering\n",
    "X_train_domain = create_domain_features(X_train)\n",
    "X_val_domain = create_domain_features(X_val)\n",
    "X_test_domain = create_domain_features(X_test)\n",
    "\n",
    "# Calculate new features added\n",
    "new_features_count = X_train_domain.shape[1] - X_train.shape[1]\n",
    "print(f\"\\n✅ Domain feature engineering completed!\")\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"New features added: {new_features_count}\")\n",
    "print(f\"Total features: {X_train_domain.shape[1]}\")\n",
    "\n",
    "# Display new features\n",
    "new_feature_names = [col for col in X_train_domain.columns if col not in X_train.columns]\n",
    "print(f\"\\n🆕 New Domain Features:\")\n",
    "for i, feature in enumerate(new_feature_names, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create statistical features\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_statistical_features(df):\n",
    "    \"\"\"\n",
    "    Create statistical features from existing numerical features\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Define feature groups for statistical analysis\n",
    "    financial_features = [col for col in df.columns if any(keyword in col.lower() \n",
    "                         for keyword in ['income', 'salary', 'debt', 'balance', 'emi', 'amount'])]\n",
    "    \n",
    "    credit_features = [col for col in df.columns if any(keyword in col.lower() \n",
    "                      for keyword in ['credit', 'num_', 'account'])]\n",
    "    \n",
    "    payment_features = [col for col in df.columns if any(keyword in col.lower() \n",
    "                       for keyword in ['payment', 'delay', 'interest'])]\n",
    "    \n",
    "    print(f\"📊 Creating statistical aggregations...\")\n",
    "    print(f\"  Financial features: {len(financial_features)}\")\n",
    "    print(f\"  Credit features: {len(credit_features)}\")\n",
    "    print(f\"  Payment features: {len(payment_features)}\")\n",
    "    \n",
    "    # Group-wise statistical features\n",
    "    feature_groups = {\n",
    "        'Financial': financial_features,\n",
    "        'Credit': credit_features,\n",
    "        'Payment': payment_features\n",
    "    }\n",
    "    \n",
    "    for group_name, feature_list in feature_groups.items():\n",
    "        if len(feature_list) >= 2:  # Need at least 2 features for meaningful stats\n",
    "            group_data = df[feature_list]\n",
    "            \n",
    "            # Statistical aggregations\n",
    "            df_new[f'{group_name}_Mean'] = group_data.mean(axis=1)\n",
    "            df_new[f'{group_name}_Std'] = group_data.std(axis=1)\n",
    "            df_new[f'{group_name}_Max'] = group_data.max(axis=1)\n",
    "            df_new[f'{group_name}_Min'] = group_data.min(axis=1)\n",
    "            df_new[f'{group_name}_Range'] = df_new[f'{group_name}_Max'] - df_new[f'{group_name}_Min']\n",
    "            \n",
    "            # Coefficient of variation (normalized variance)\n",
    "            df_new[f'{group_name}_CV'] = np.where(\n",
    "                df_new[f'{group_name}_Mean'] != 0,\n",
    "                df_new[f'{group_name}_Std'] / df_new[f'{group_name}_Mean'],\n",
    "                0\n",
    "            )\n",
    "    \n",
    "    # Mathematical transformations\n",
    "    print(f\"🔢 Creating mathematical transformations...\")\n",
    "    \n",
    "    # Log transformations for skewed features\n",
    "    skewed_features = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].min() >= 0:  # Only for non-negative features\n",
    "            skewness = abs(skew(df[col]))\n",
    "            if skewness > 1:  # Highly skewed\n",
    "                skewed_features.append(col)\n",
    "                df_new[f'{col}_Log'] = np.log1p(df[col])  # log(1 + x) to handle zeros\n",
    "    \n",
    "    print(f\"  Log-transformed features: {len(skewed_features)}\")\n",
    "    \n",
    "    # Square root transformations\n",
    "    sqrt_candidates = ['Age', 'Credit_History_Age', 'Num_of_Loan']\n",
    "    for col in sqrt_candidates:\n",
    "        if col in df.columns:\n",
    "            df_new[f'{col}_Sqrt'] = np.sqrt(df[col])\n",
    "    \n",
    "    # Binning of continuous features\n",
    "    print(f\"📦 Creating binned features...\")\n",
    "    \n",
    "    binning_features = {\n",
    "        'Age': [18, 30, 45, 60, 100],\n",
    "        'Credit_History_Age': [0, 12, 60, 120, 300],\n",
    "        'Annual_Income': 5  # Use quantile-based binning\n",
    "    }\n",
    "    \n",
    "    for feature, bins in binning_features.items():\n",
    "        if feature in df.columns:\n",
    "            if isinstance(bins, list):\n",
    "                df_new[f'{feature}_Binned'] = pd.cut(df[feature], bins=bins, labels=False)\n",
    "            else:  # Quantile-based binning\n",
    "                df_new[f'{feature}_Binned'] = pd.qcut(df[feature], q=bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply statistical feature engineering\n",
    "X_train_stats = create_statistical_features(X_train_domain)\n",
    "X_val_stats = create_statistical_features(X_val_domain)\n",
    "X_test_stats = create_statistical_features(X_test_domain)\n",
    "\n",
    "# Calculate new features added\n",
    "stats_features_count = X_train_stats.shape[1] - X_train_domain.shape[1]\n",
    "print(f\"\\n✅ Statistical feature engineering completed!\")\n",
    "print(f\"Previous features: {X_train_domain.shape[1]}\")\n",
    "print(f\"New statistical features: {stats_features_count}\")\n",
    "print(f\"Total features: {X_train_stats.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "print(\"=\" * 60)\n",
    "print(\"INTERACTION FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create meaningful interaction features based on domain knowledge\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    print(\"🔗 Creating interaction features...\")\n",
    "    \n",
    "    # High-value interactions based on financial domain knowledge\n",
    "    interactions = [\n",
    "        # Income and credit interactions\n",
    "        ('Annual_Income', 'Num_Credit_Card'),\n",
    "        ('Monthly_Inhand_Salary', 'Credit_Utilization_Ratio'),\n",
    "        ('Annual_Income', 'Outstanding_Debt'),\n",
    "        \n",
    "        # Age and experience interactions\n",
    "        ('Age', 'Credit_History_Age'),\n",
    "        ('Age', 'Num_Bank_Accounts'),\n",
    "        \n",
    "        # Payment behavior interactions\n",
    "        ('Delay_from_due_date', 'Num_of_Delayed_Payment'),\n",
    "        ('Interest_Rate', 'Outstanding_Debt'),\n",
    "        \n",
    "        # Credit mix interactions\n",
    "        ('Num_Credit_Card', 'Credit_Utilization_Ratio'),\n",
    "        ('Num_of_Loan', 'Total_EMI_per_month'),\n",
    "    ]\n",
    "    \n",
    "    interaction_count = 0\n",
    "    for feat1, feat2 in interactions:\n",
    "        if feat1 in df.columns and feat2 in df.columns:\n",
    "            # Multiplicative interaction\n",
    "            df_new[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]\n",
    "            \n",
    "            # Ratio interaction (if denominator is not zero)\n",
    "            df_new[f'{feat1}_div_{feat2}'] = np.where(\n",
    "                df[feat2] != 0,\n",
    "                df[feat1] / df[feat2],\n",
    "                0\n",
    "            )\n",
    "            \n",
    "            # Difference interaction\n",
    "            df_new[f'{feat1}_minus_{feat2}'] = df[feat1] - df[feat2]\n",
    "            \n",
    "            interaction_count += 3\n",
    "    \n",
    "    print(f\"  Created {interaction_count} interaction features from {len(interactions)} feature pairs\")\n",
    "    \n",
    "    # Create polynomial features for key financial ratios\n",
    "    print(\"📐 Creating polynomial features...\")\n",
    "    \n",
    "    key_ratios = []\n",
    "    for col in df.columns:\n",
    "        if 'ratio' in col.lower() or 'rate' in col.lower():\n",
    "            key_ratios.append(col)\n",
    "    \n",
    "    polynomial_count = 0\n",
    "    for ratio in key_ratios[:5]:  # Limit to top 5 to avoid explosion\n",
    "        if ratio in df.columns:\n",
    "            df_new[f'{ratio}_Squared'] = df[ratio] ** 2\n",
    "            df_new[f'{ratio}_Cubed'] = df[ratio] ** 3\n",
    "            polynomial_count += 2\n",
    "    \n",
    "    print(f\"  Created {polynomial_count} polynomial features\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply interaction feature engineering\n",
    "X_train_inter = create_interaction_features(X_train_stats)\n",
    "X_val_inter = create_interaction_features(X_val_stats)\n",
    "X_test_inter = create_interaction_features(X_test_stats)\n",
    "\n",
    "# Calculate new features added\n",
    "interaction_features_count = X_train_inter.shape[1] - X_train_stats.shape[1]\n",
    "print(f\"\\n✅ Interaction feature engineering completed!\")\n",
    "print(f\"Previous features: {X_train_stats.shape[1]}\")\n",
    "print(f\"New interaction features: {interaction_features_count}\")\n",
    "print(f\"Total features: {X_train_inter.shape[1]}\")\n",
    "\n",
    "# Check for any infinite or NaN values\n",
    "inf_check = np.isinf(X_train_inter).sum().sum()\n",
    "nan_check = np.isnan(X_train_inter).sum().sum()\n",
    "\n",
    "if inf_check > 0 or nan_check > 0:\n",
    "    print(f\"⚠️  Found {inf_check} infinite and {nan_check} NaN values - cleaning...\")\n",
    "    # Replace inf with large finite values and NaN with 0\n",
    "    X_train_inter = X_train_inter.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_val_inter = X_val_inter.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    X_test_inter = X_test_inter.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    print(\"✅ Cleaned infinite and NaN values\")\n",
    "else:\n",
    "    print(\"✅ No infinite or NaN values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature selection to identify most important features\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Current feature count\n",
    "total_features = X_train_inter.shape[1]\n",
    "print(f\"Total engineered features: {total_features}\")\n",
    "\n",
    "# Target number of features to select\n",
    "target_features = min(50, total_features // 2)  # Select up to 50 features or half of total\n",
    "print(f\"Target features for selection: {target_features}\")\n",
    "\n",
    "# Method 1: Univariate Feature Selection (F-test)\n",
    "print(\"\\n🔍 Method 1: Univariate F-test Selection\")\n",
    "selector_f = SelectKBest(score_func=f_classif, k=target_features)\n",
    "selector_f.fit(X_train_inter, y_train)\n",
    "\n",
    "# Get feature scores\n",
    "f_scores = pd.DataFrame({\n",
    "    'Feature': X_train_inter.columns,\n",
    "    'F_Score': selector_f.scores_,\n",
    "    'P_Value': selector_f.pvalues_\n",
    "}).sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(f\"Top 10 features by F-test:\")\n",
    "print(f_scores.head(10))\n",
    "\n",
    "# Method 2: Mutual Information Selection\n",
    "print(\"\\n🔍 Method 2: Mutual Information Selection\")\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=target_features)\n",
    "selector_mi.fit(X_train_inter, y_train)\n",
    "\n",
    "mi_scores = pd.DataFrame({\n",
    "    'Feature': X_train_inter.columns,\n",
    "    'MI_Score': selector_mi.scores_\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(f\"Top 10 features by Mutual Information:\")\n",
    "print(mi_scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Tree-based Feature Importance\n",
    "print(\"\\n🔍 Method 3: Tree-based Feature Importance\")\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_selector.fit(X_train_inter, y_train)\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train_inter.columns,\n",
    "    'RF_Importance': rf_selector.feature_importances_\n",
    "}).sort_values('RF_Importance', ascending=False)\n",
    "\n",
    "print(f\"Top 10 features by Random Forest importance:\")\n",
    "print(rf_importance.head(10))\n",
    "\n",
    "# Method 4: Recursive Feature Elimination\n",
    "print(\"\\n🔍 Method 4: Recursive Feature Elimination (RFE)\")\n",
    "\n",
    "# Use a simpler model for RFE to avoid overfitting\n",
    "rfe_estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rfe_selector = RFE(estimator=rfe_estimator, n_features_to_select=target_features)\n",
    "rfe_selector.fit(X_train_inter, y_train)\n",
    "\n",
    "rfe_selected = X_train_inter.columns[rfe_selector.support_]\n",
    "print(f\"RFE selected {len(rfe_selected)} features\")\n",
    "\n",
    "# Combine selection methods\n",
    "print(\"\\n🎯 Combining Feature Selection Methods\")\n",
    "\n",
    "# Get top features from each method\n",
    "top_k = target_features // 2  # Take top half from each method\n",
    "\n",
    "f_test_features = set(f_scores.head(top_k)['Feature'].tolist())\n",
    "mi_features = set(mi_scores.head(top_k)['Feature'].tolist())\n",
    "rf_features = set(rf_importance.head(top_k)['Feature'].tolist())\n",
    "rfe_features = set(rfe_selected)\n",
    "\n",
    "# Find features that appear in multiple methods\n",
    "feature_votes = {}\n",
    "all_features = f_test_features | mi_features | rf_features | rfe_features\n",
    "\n",
    "for feature in all_features:\n",
    "    votes = 0\n",
    "    if feature in f_test_features: votes += 1\n",
    "    if feature in mi_features: votes += 1\n",
    "    if feature in rf_features: votes += 1\n",
    "    if feature in rfe_features: votes += 1\n",
    "    feature_votes[feature] = votes\n",
    "\n",
    "# Sort by number of votes\n",
    "sorted_features = sorted(feature_votes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select features with at least 2 votes\n",
    "consensus_features = [feature for feature, votes in sorted_features if votes >= 2]\n",
    "\n",
    "# If we don't have enough consensus features, add top-voted ones\n",
    "if len(consensus_features) < target_features:\n",
    "    additional_needed = target_features - len(consensus_features)\n",
    "    additional_features = [feature for feature, votes in sorted_features[len(consensus_features):len(consensus_features)+additional_needed]]\n",
    "    consensus_features.extend(additional_features)\n",
    "\n",
    "# Limit to target number\n",
    "selected_features = consensus_features[:target_features]\n",
    "\n",
    "print(f\"\\n✅ Feature Selection Summary:\")\n",
    "print(f\"  Original features: {total_features}\")\n",
    "print(f\"  Selected features: {len(selected_features)}\")\n",
    "print(f\"  Reduction: {(1 - len(selected_features)/total_features)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🏆 Top 15 Selected Features:\")\n",
    "for i, (feature, votes) in enumerate(sorted_features[:15], 1):\n",
    "    status = \"✓\" if feature in selected_features else \"-\"\n",
    "    print(f\"  {i:2d}. {status} {feature} ({votes}/4 votes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Final Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final datasets with selected features\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING FINAL FEATURE SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_final = X_train_inter[selected_features].copy()\n",
    "X_val_final = X_val_inter[selected_features].copy()\n",
    "X_test_final = X_test_inter[selected_features].copy()\n",
    "\n",
    "print(f\"📊 Final Feature Set:\")\n",
    "print(f\"  Training set: {X_train_final.shape}\")\n",
    "print(f\"  Validation set: {X_val_final.shape}\")\n",
    "print(f\"  Test set: {X_test_final.shape}\")\n",
    "\n",
    "# Analyze feature types in final set\n",
    "original_features = [f for f in selected_features if f in feature_info['feature_names']]\n",
    "domain_features = [f for f in selected_features if any(keyword in f for keyword in \n",
    "                  ['Debt_to_Income', 'Savings_Rate', 'Payment_Reliability', 'Financial_Stress', \n",
    "                   'Credit_Experience', 'Income_Stability', 'Credit_Utilization_Efficiency'])]\n",
    "statistical_features = [f for f in selected_features if any(keyword in f for keyword in \n",
    "                       ['_Mean', '_Std', '_Max', '_Min', '_CV', '_Log', '_Sqrt', '_Binned'])]\n",
    "interaction_features = [f for f in selected_features if any(keyword in f for keyword in \n",
    "                       ['_x_', '_div_', '_minus_', '_Squared', '_Cubed'])]\n",
    "\n",
    "print(f\"\\n🔍 Feature Composition:\")\n",
    "print(f\"  Original features: {len(original_features)} ({len(original_features)/len(selected_features)*100:.1f}%)\")\n",
    "print(f\"  Domain features: {len(domain_features)} ({len(domain_features)/len(selected_features)*100:.1f}%)\")\n",
    "print(f\"  Statistical features: {len(statistical_features)} ({len(statistical_features)/len(selected_features)*100:.1f}%)\")\n",
    "print(f\"  Interaction features: {len(interaction_features)} ({len(interaction_features)/len(selected_features)*100:.1f}%)\")\n",
    "\n",
    "# Display feature categories\n",
    "feature_categories = {\n",
    "    'Original': original_features,\n",
    "    'Domain': domain_features,\n",
    "    'Statistical': statistical_features,\n",
    "    'Interaction': interaction_features\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"\\n📋 {category} Features ({len(features)}):\")\n",
    "        for i, feature in enumerate(features, 1):\n",
    "            print(f\"  {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Validation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the final feature set\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE VALIDATION AND ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for multicollinearity in final feature set\n",
    "print(\"🔍 Checking for multicollinearity...\")\n",
    "correlation_matrix_final = X_train_final.corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_threshold = 0.9\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix_final.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix_final.columns)):\n",
    "        corr_value = correlation_matrix_final.iloc[i, j]\n",
    "        if abs(corr_value) > high_corr_threshold:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature_1': correlation_matrix_final.columns[i],\n",
    "                'Feature_2': correlation_matrix_final.columns[j],\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"⚠️  Found {len(high_corr_pairs)} highly correlated pairs (|r| > {high_corr_threshold}):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"  {pair['Feature_1']} ↔ {pair['Feature_2']}: {pair['Correlation']:.3f}\")\n",
    "else:\n",
    "    print(f\"✅ No highly correlated feature pairs found (threshold: {high_corr_threshold})\")\n",
    "\n",
    "# Quick model validation with selected features\n",
    "print(\"\\n🎯 Quick Model Validation...\")\n",
    "\n",
    "# Train a simple model to validate feature quality\n",
    "validation_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "validation_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred = validation_model.predict(X_val_final)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation accuracy with selected features: {validation_accuracy:.4f}\")\n",
    "\n",
    "# Compare with original features (subset)\n",
    "original_subset = [f for f in original_features if f in X_train.columns][:len(selected_features)//2]\n",
    "if original_subset:\n",
    "    baseline_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    baseline_model.fit(X_train[original_subset], y_train)\n",
    "    baseline_pred = baseline_model.predict(X_val[original_subset])\n",
    "    baseline_accuracy = accuracy_score(y_val, baseline_pred)\n",
    "    \n",
    "    improvement = (validation_accuracy - baseline_accuracy) / baseline_accuracy * 100\n",
    "    print(f\"Baseline accuracy with original features: {baseline_accuracy:.4f}\")\n",
    "    print(f\"Performance improvement: {improvement:+.2f}%\")\n",
    "\n",
    "# Feature importance in final model\n",
    "final_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': validation_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n🏆 Top 10 Most Important Features in Final Model:\")\n",
    "print(final_importance.head(10))\n",
    "\n",
    "# Visualize top feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features_plot = final_importance.head(15)\n",
    "plt.barh(range(len(top_features_plot)), top_features_plot['Importance'])\n",
    "plt.yticks(range(len(top_features_plot)), top_features_plot['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances in Final Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Feature-Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature-engineered datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING FEATURE-ENGINEERED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create directory for feature-engineered data\n",
    "engineered_data_dir = Path(\"../data/engineered\")\n",
    "engineered_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save final datasets\n",
    "datasets_final = {\n",
    "    'X_train_engineered': X_train_final,\n",
    "    'X_val_engineered': X_val_final,\n",
    "    'X_test_engineered': X_test_final,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets_final.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        filepath = engineered_data_dir / f\"{name}.csv\"\n",
    "        data.to_csv(filepath, index=False)\n",
    "    else:  # numpy array\n",
    "        filepath = engineered_data_dir / f\"{name}.csv\"\n",
    "        pd.Series(data).to_csv(filepath, index=False, header=[name])\n",
    "    \n",
    "    print(f\"✅ {name} saved: {filepath}\")\n",
    "\n",
    "# Save feature engineering metadata\n",
    "feature_engineering_info = {\n",
    "    'selected_features': selected_features,\n",
    "    'feature_categories': feature_categories,\n",
    "    'total_features_before_selection': total_features,\n",
    "    'total_features_after_selection': len(selected_features),\n",
    "    'feature_selection_methods': ['f_test', 'mutual_info', 'random_forest', 'rfe'],\n",
    "    'validation_accuracy': validation_accuracy,\n",
    "    'target_classes': feature_info['target_classes']\n",
    "}\n",
    "\n",
    "feature_engineering_path = MODELS_DIR / \"feature_engineering_info.joblib\"\n",
    "joblib.dump(feature_engineering_info, feature_engineering_path)\n",
    "print(f\"✅ Feature engineering info saved: {feature_engineering_path}\")\n",
    "\n",
    "# Save feature selection results for future reference\n",
    "feature_selection_results = {\n",
    "    'f_scores': f_scores,\n",
    "    'mi_scores': mi_scores,\n",
    "    'rf_importance': rf_importance,\n",
    "    'final_importance': final_importance,\n",
    "    'correlation_matrix': correlation_matrix_final\n",
    "}\n",
    "\n",
    "selection_results_path = MODELS_DIR / \"feature_selection_results.joblib\"\n",
    "joblib.dump(feature_selection_results, selection_results_path)\n",
    "print(f\"✅ Feature selection results saved: {selection_results_path}\")\n",
    "\n",
    "# Create feature engineering summary\n",
    "engineering_summary = f\"\"\"\n",
    "# Feature Engineering Summary Report\n",
    "\n",
    "## Dataset Information\n",
    "- **Input data**: Preprocessed datasets\n",
    "- **Output data**: {engineered_data_dir}\n",
    "- **Final training samples**: {len(X_train_final)}\n",
    "- **Final features**: {len(selected_features)}\n",
    "- **Validation accuracy**: {validation_accuracy:.4f}\n",
    "\n",
    "## Feature Engineering Process\n",
    "1. ✅ Domain knowledge features: {new_features_count} features\n",
    "2. ✅ Statistical features: {stats_features_count} features\n",
    "3. ✅ Interaction features: {interaction_features_count} features\n",
    "4. ✅ Feature selection: {total_features} → {len(selected_features)} features\n",
    "5. ✅ Feature validation and analysis\n",
    "6. ✅ Export engineered datasets\n",
    "\n",
    "## Feature Composition\n",
    "- **Original features**: {len(original_features)} ({len(original_features)/len(selected_features)*100:.1f}%)\n",
    "- **Domain features**: {len(domain_features)} ({len(domain_features)/len(selected_features)*100:.1f}%)\n",
    "- **Statistical features**: {len(statistical_features)} ({len(statistical_features)/len(selected_features)*100:.1f}%)\n",
    "- **Interaction features**: {len(interaction_features)} ({len(interaction_features)/len(selected_features)*100:.1f}%)\n",
    "\n",
    "## Feature Selection Methods\n",
    "- F-test (univariate)\n",
    "- Mutual Information\n",
    "- Random Forest importance\n",
    "- Recursive Feature Elimination\n",
    "\n",
    "## Top 5 Most Important Features\n",
    "{final_importance.head(5).to_string(index=False)}\n",
    "\n",
    "## Quality Metrics\n",
    "- **Highly correlated pairs**: {len(high_corr_pairs)}\n",
    "- **Feature reduction**: {(1 - len(selected_features)/total_features)*100:.1f}%\n",
    "- **Memory usage**: {X_train_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
    "\n",
    "## Next Steps\n",
    "Proceed to the **Model Building Notebook** for training and evaluating machine learning models.\n",
    "\"\"\"\n",
    "\n",
    "print(engineering_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 Notebook Summary\n",
    "\n",
    "This notebook has successfully completed the feature engineering process for the credit score classification dataset. The key accomplishments include:\n",
    "\n",
    "### ✅ **Completed Tasks:**\n",
    "1. **Data Loading**: Successfully loaded preprocessed datasets and encoders\n",
    "2. **Domain Knowledge Features**: Created financial domain-specific features\n",
    "3. **Statistical Features**: Generated statistical aggregations and transformations\n",
    "4. **Interaction Features**: Created meaningful feature interactions\n",
    "5. **Feature Selection**: Applied multiple selection methods to identify the best features\n",
    "6. **Feature Validation**: Validated feature quality and model performance\n",
    "7. **Quality Assurance**: Checked for multicollinearity and data quality issues\n",
    "8. **Data Export**: Saved feature-engineered datasets and metadata\n",
    "\n",
    "### 🎯 **Key Outcomes:**\n",
    "- Enhanced dataset with domain-specific financial features\n",
    "- Optimized feature set through rigorous selection process\n",
    "- Improved model performance through feature engineering\n",
    "- Comprehensive feature analysis and documentation\n",
    "\n",
    "### 📊 **Feature Engineering Results:**\n",
    "- **Original features**: Baseline financial and demographic data\n",
    "- **Domain features**: Financial health indicators, credit behavior metrics\n",
    "- **Statistical features**: Aggregations, transformations, and binning\n",
    "- **Interaction features**: Meaningful feature combinations and polynomials\n",
    "\n",
    "### ➡️ **Next Step:**\n",
    "Proceed to the **Model Building Notebook** to train and evaluate machine learning models using the engineered features.\n",
    "\n",
    "---"
   ]
  }\n ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}