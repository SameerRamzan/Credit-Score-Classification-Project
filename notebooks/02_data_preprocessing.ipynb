{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Score Classification - Data Preprocessing Notebook\n",
    "\n",
    "## Objective\n",
    "This notebook performs comprehensive data preprocessing on the cleaned credit score dataset to prepare it for machine learning model training.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Data Loading**: Load the cleaned dataset from the previous step\n",
    "2. **Feature Selection**: Identify and select relevant features for modeling\n",
    "3. **Categorical Encoding**: Convert categorical variables to numerical format\n",
    "4. **Feature Scaling**: Normalize numerical features for consistent scaling\n",
    "5. **Data Splitting**: Split data into training, validation, and testing sets\n",
    "6. **Class Balancing**: Handle class imbalance if necessary\n",
    "7. **Correlation Analysis**: Analyze feature correlations and multicollinearity\n",
    "8. **Data Validation**: Ensure preprocessing quality\n",
    "9. **Export Processed Data**: Save preprocessed datasets for model training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Class balancing\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import CLEANED_DATA_DIR, MODELS_DIR, PREPROCESSING_CONFIG\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plot styles\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "data_path = CLEANED_DATA_DIR / \"credit_score_cleaned.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"📊 Cleaned dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for modeling\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify features that should be excluded from modeling\n",
    "exclude_features = [\n",
    "    'ID', 'Customer_ID', 'Name', 'SSN',  # Identifier columns\n",
    "    'Month',  # Time-based feature (not needed for this model)\n",
    "    'Type_of_Loan'  # Too many categories, will be handled separately if needed\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target_column = 'Credit_Score'\n",
    "\n",
    "# Available features (excluding target and identifier columns)\n",
    "available_features = [col for col in df.columns if col not in exclude_features + [target_column]]\n",
    "\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Excluded columns: {exclude_features}\")\n",
    "print(f\"Target column: {target_column}\")\n",
    "print(f\"Available features ({len(available_features)}): {available_features}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df[available_features].copy()\n",
    "y = df[target_column].copy()\n",
    "\n",
    "print(f\"\\n📊 Feature matrix shape: {X.shape}\")\n",
    "print(f\"🎯 Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\n📊 Numerical features ({len(numerical_features)}):\")\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\n📝 Categorical features ({len(categorical_features)}):\")\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    unique_count = X[feature].nunique()\n",
    "    print(f\"  {i:2d}. {feature} ({unique_count} unique values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of numerical features\n",
    "print(\"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Statistical summary\n",
    "print(X[numerical_features].describe())\n",
    "\n",
    "# Visualize distributions\n",
    "n_cols = 3\n",
    "n_rows = (len(numerical_features) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten() if n_rows > 1 else [axes]\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(X[feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'Distribution: {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(numerical_features), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n📝 {feature}:\")\n",
    "    value_counts = X[feature].value_counts()\n",
    "    print(value_counts)\n",
    "    \n",
    "    # Show percentage distribution\n",
    "    percentages = X[feature].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage distribution:\")\n",
    "    for value, pct in percentages.items():\n",
    "        print(f\"  {value}: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply categorical encoding\n",
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "X_processed = X.copy()\n",
    "\n",
    "# Dictionary to store encoders for later use\n",
    "encoders = {}\n",
    "\n",
    "# Encode categorical features\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n🔧 Encoding {feature}...\")\n",
    "    \n",
    "    unique_values = X_processed[feature].nunique()\n",
    "    print(f\"  Unique values: {unique_values}\")\n",
    "    \n",
    "    if unique_values <= 10:  # Use Label Encoding for features with few categories\n",
    "        le = LabelEncoder()\n",
    "        X_processed[feature] = le.fit_transform(X_processed[feature])\n",
    "        encoders[feature] = {'type': 'label', 'encoder': le}\n",
    "        print(f\"  Applied: Label Encoding\")\n",
    "        print(f\"  Classes: {list(le.classes_)}\")\n",
    "    \n",
    "    else:  # Use One-Hot Encoding for features with many categories\n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(X_processed[feature], prefix=feature)\n",
    "        \n",
    "        # Drop original column and add dummy columns\n",
    "        X_processed = X_processed.drop(feature, axis=1)\n",
    "        X_processed = pd.concat([X_processed, dummies], axis=1)\n",
    "        \n",
    "        encoders[feature] = {'type': 'onehot', 'columns': list(dummies.columns)}\n",
    "        print(f\"  Applied: One-Hot Encoding\")\n",
    "        print(f\"  Created columns: {list(dummies.columns)}\")\n",
    "\n",
    "print(f\"\\n✅ Categorical encoding completed!\")\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "\n",
    "# Update feature lists\n",
    "numerical_features_final = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Final number of features: {len(numerical_features_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET VARIABLE ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Current target distribution\n",
    "print(\"Original target distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"\\n🔧 Target variable encoded:\")\n",
    "print(f\"Classes: {list(target_encoder.classes_)}\")\n",
    "print(f\"Encoded values: {dict(zip(target_encoder.classes_, target_encoder.transform(target_encoder.classes_)))}\")\n",
    "\n",
    "# Show encoded target distribution\n",
    "y_encoded_series = pd.Series(y_encoded)\n",
    "print(f\"\\nEncoded target distribution:\")\n",
    "for encoded_val, count in y_encoded_series.value_counts().sort_index().items():\n",
    "    original_label = target_encoder.inverse_transform([encoded_val])[0]\n",
    "    print(f\"  {encoded_val} ({original_label}): {count}\")\n",
    "\n",
    "# Store target encoder\n",
    "encoders['target'] = target_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the need for scaling\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SCALING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check feature scales\n",
    "feature_stats = X_processed[numerical_features_final].describe().T\n",
    "feature_stats['range'] = feature_stats['max'] - feature_stats['min']\n",
    "feature_stats['scale_difference'] = feature_stats['max'] / feature_stats['min']\n",
    "\n",
    "print(\"Feature scale analysis:\")\n",
    "print(feature_stats[['min', 'max', 'range', 'scale_difference']].round(2))\n",
    "\n",
    "# Identify features that need scaling\n",
    "features_needing_scaling = feature_stats[feature_stats['scale_difference'] > 10].index.tolist()\n",
    "print(f\"\\n🔍 Features with high scale differences (>10x): {len(features_needing_scaling)}\")\n",
    "for feature in features_needing_scaling:\n",
    "    scale_diff = feature_stats.loc[feature, 'scale_difference']\n",
    "    print(f\"  {feature}: {scale_diff:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature scaling\n",
    "print(\"\\n🔧 Applying feature scaling...\")\n",
    "\n",
    "# Choose scaling method based on data distribution\n",
    "scaling_method = PREPROCESSING_CONFIG['scaling_method']\n",
    "\n",
    "if scaling_method == 'StandardScaler':\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Using StandardScaler (mean=0, std=1)\")\n",
    "elif scaling_method == 'MinMaxScaler':\n",
    "    scaler = MinMaxScaler()\n",
    "    print(\"Using MinMaxScaler (range 0-1)\")\n",
    "elif scaling_method == 'RobustScaler':\n",
    "    scaler = RobustScaler()\n",
    "    print(\"Using RobustScaler (median and IQR)\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    print(\"Using default StandardScaler\")\n",
    "\n",
    "# Apply scaling to numerical features\n",
    "X_scaled = X_processed.copy()\n",
    "X_scaled[numerical_features_final] = scaler.fit_transform(X_processed[numerical_features_final])\n",
    "\n",
    "print(f\"✅ Feature scaling completed!\")\n",
    "print(f\"Scaled features: {len(numerical_features_final)}\")\n",
    "\n",
    "# Store scaler\n",
    "encoders['scaler'] = scaler\n",
    "\n",
    "# Show scaling effect\n",
    "print(\"\\nScaling effect (first 5 features):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Feature': numerical_features_final[:5],\n",
    "    'Original_Mean': X_processed[numerical_features_final[:5]].mean().round(2),\n",
    "    'Original_Std': X_processed[numerical_features_final[:5]].std().round(2),\n",
    "    'Scaled_Mean': X_scaled[numerical_features_final[:5]].mean().round(6),\n",
    "    'Scaled_Std': X_scaled[numerical_features_final[:5]].std().round(6)\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis and Multicollinearity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_scaled[numerical_features_final].corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > high_corr_threshold:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature_1': correlation_matrix.columns[i],\n",
    "                'Feature_2': correlation_matrix.columns[j],\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"⚠️  Found {len(high_corr_pairs)} highly correlated feature pairs (|correlation| > {high_corr_threshold}):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"  {pair['Feature_1']} ↔ {pair['Feature_2']}: {pair['Correlation']:.3f}\")\n",
    "else:\n",
    "    print(f\"✅ No highly correlated feature pairs found (threshold: {high_corr_threshold})\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "test_size = 0.2\n",
    "val_size = 0.2  # 20% of training data\n",
    "random_state = 42\n",
    "\n",
    "# First split: separate test set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Second split: separate training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=val_size, \n",
    "    random_state=random_state, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"📊 Data splitting completed:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_scaled)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(X_scaled)} samples\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "def print_class_distribution(y_set, set_name):\n",
    "    \"\"\"Print class distribution for a dataset split\"\"\"\n",
    "    unique, counts = np.unique(y_set, return_counts=True)\n",
    "    percentages = counts / len(y_set) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} class distribution:\")\n",
    "    for class_val, count, pct in zip(unique, counts, percentages):\n",
    "        original_label = target_encoder.inverse_transform([class_val])[0]\n",
    "        print(f\"  Class {class_val} ({original_label}): {count} ({pct:.1f}%)\")\n",
    "\n",
    "print_class_distribution(y_train, \"Training\")\n",
    "print_class_distribution(y_val, \"Validation\")\n",
    "print_class_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Class Balancing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "max_count = max(counts)\n",
    "min_count = min(counts)\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"Class distribution in training set:\")\n",
    "for class_val, count in zip(unique, counts):\n",
    "    original_label = target_encoder.inverse_transform([class_val])[0]\n",
    "    percentage = count / len(y_train) * 100\n",
    "    print(f\"  Class {class_val} ({original_label}): {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Apply class balancing if needed\n",
    "balance_threshold = 3.0  # Apply balancing if ratio > 3:1\n",
    "\n",
    "if imbalance_ratio > balance_threshold:\n",
    "    print(f\"\\n⚠️  Class imbalance detected (ratio > {balance_threshold}:1)\")\n",
    "    print(\"Applying SMOTE for class balancing...\")\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"\\n✅ Class balancing completed:\")\n",
    "    print(f\"  Original training samples: {len(X_train)}\")\n",
    "    print(f\"  Balanced training samples: {len(X_train_balanced)}\")\n",
    "    \n",
    "    # Show new distribution\n",
    "    print_class_distribution(y_train_balanced, \"Balanced Training\")\n",
    "    \n",
    "    # Use balanced data for training\n",
    "    X_train_final = X_train_balanced\n",
    "    y_train_final = y_train_balanced\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n✅ Class distribution is acceptable (ratio ≤ {balance_threshold}:1)\")\n",
    "    print(\"No class balancing applied.\")\n",
    "    \n",
    "    # Use original data\n",
    "    X_train_final = X_train\n",
    "    y_train_final = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance using simple correlation with target\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate correlation with target for numerical features\n",
    "target_correlations = []\n",
    "\n",
    "for feature in numerical_features_final:\n",
    "    # Use training data for correlation analysis\n",
    "    feature_data = X_train_final[feature]\n",
    "    \n",
    "    # Calculate Pearson correlation\n",
    "    corr_coef, p_value = stats.pearsonr(feature_data, y_train_final)\n",
    "    \n",
    "    target_correlations.append({\n",
    "        'Feature': feature,\n",
    "        'Correlation': abs(corr_coef),  # Use absolute value\n",
    "        'P_Value': p_value,\n",
    "        'Significant': p_value < 0.05\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by correlation\n",
    "feature_importance_df = pd.DataFrame(target_correlations)\n",
    "feature_importance_df = feature_importance_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"Feature correlation with target (sorted by absolute correlation):\")\n",
    "print(feature_importance_df.head(15))  # Show top 15 features\n",
    "\n",
    "# Visualize top features\n",
    "top_features = feature_importance_df.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_features)), top_features['Correlation'], \n",
    "         color=['green' if sig else 'orange' for sig in top_features['Significant']])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.title('Top 10 Features by Correlation with Target')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='green', label='Significant (p < 0.05)'),\n",
    "                   Patch(facecolor='orange', label='Not Significant (p ≥ 0.05)')]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify most important features\n",
    "important_features = feature_importance_df[\n",
    "    (feature_importance_df['Correlation'] > 0.1) & \n",
    "    (feature_importance_df['Significant'] == True)\n",
    "]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n🎯 Most important features ({len(important_features)}):\")\n",
    "for i, feature in enumerate(important_features, 1):\n",
    "    corr = feature_importance_df[feature_importance_df['Feature'] == feature]['Correlation'].iloc[0]\n",
    "    print(f\"  {i:2d}. {feature} (correlation: {corr:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final data validation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA VALIDATION AND QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "# Check 1: No missing values\n",
    "missing_train = X_train_final.isnull().sum().sum()\n",
    "missing_val = X_val.isnull().sum().sum()\n",
    "missing_test = X_test.isnull().sum().sum()\n",
    "\n",
    "if missing_train == 0 and missing_val == 0 and missing_test == 0:\n",
    "    validation_results.append(\"✅ No missing values in any dataset\")\n",
    "else:\n",
    "    validation_results.append(f\"❌ Missing values found - Train: {missing_train}, Val: {missing_val}, Test: {missing_test}\")\n",
    "\n",
    "# Check 2: Feature consistency across sets\n",
    "train_features = set(X_train_final.columns)\n",
    "val_features = set(X_val.columns)\n",
    "test_features = set(X_test.columns)\n",
    "\n",
    "if train_features == val_features == test_features:\n",
    "    validation_results.append(\"✅ Feature consistency across all datasets\")\n",
    "else:\n",
    "    validation_results.append(\"❌ Feature inconsistency detected across datasets\")\n",
    "\n",
    "# Check 3: Data types are numeric\n",
    "numeric_check = all(X_train_final.dtypes.apply(pd.api.types.is_numeric_dtype))\n",
    "if numeric_check:\n",
    "    validation_results.append(\"✅ All features are numeric\")\n",
    "else:\n",
    "    validation_results.append(\"❌ Non-numeric features detected\")\n",
    "\n",
    "# Check 4: Target encoding is correct\n",
    "target_range = (y_train_final.min(), y_train_final.max())\n",
    "expected_range = (0, len(target_encoder.classes_) - 1)\n",
    "if target_range == expected_range:\n",
    "    validation_results.append(f\"✅ Target encoding is correct (range: {target_range})\")\n",
    "else:\n",
    "    validation_results.append(f\"❌ Target encoding issue - Range: {target_range}, Expected: {expected_range}\")\n",
    "\n",
    "# Check 5: Scaling is applied correctly\n",
    "scaled_means = X_train_final[numerical_features_final].mean()\n",
    "scaled_stds = X_train_final[numerical_features_final].std()\n",
    "\n",
    "if scaling_method == 'StandardScaler':\n",
    "    mean_check = all(abs(scaled_means) < 1e-10)  # Should be close to 0\n",
    "    std_check = all(abs(scaled_stds - 1) < 1e-10)  # Should be close to 1\n",
    "    if mean_check and std_check:\n",
    "        validation_results.append(\"✅ StandardScaler applied correctly\")\n",
    "    else:\n",
    "        validation_results.append(\"⚠️  StandardScaler may not be applied correctly\")\n",
    "\n",
    "# Display validation results\n",
    "for result in validation_results:\n",
    "    print(result)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📊 Final Dataset Summary:\")\n",
    "print(f\"  Training samples: {len(X_train_final)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Total features: {X_train_final.shape[1]}\")\n",
    "print(f\"  Target classes: {len(target_encoder.classes_)}\")\n",
    "print(f\"  Memory usage: {X_train_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Preprocessed Data and Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_dir = Path(\"data/processed\")\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING PREPROCESSED DATA AND ENCODERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save datasets\n",
    "datasets = {\n",
    "    'X_train': X_train_final,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train_final,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        filepath = processed_data_dir / f\"{name}.csv\"\n",
    "        data.to_csv(filepath, index=False)\n",
    "    else:  # numpy array or series\n",
    "        filepath = processed_data_dir / f\"{name}.csv\"\n",
    "        pd.Series(data).to_csv(filepath, index=False, header=[name])\n",
    "    \n",
    "    print(f\"✅ {name} saved: {filepath}\")\n",
    "\n",
    "# Save encoders and preprocessors\n",
    "preprocessor_path = MODELS_DIR / \"preprocessors.joblib\"\n",
    "joblib.dump(encoders, preprocessor_path)\n",
    "print(f\"✅ Encoders saved: {preprocessor_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    'feature_names': list(X_train_final.columns),\n",
    "    'numerical_features': numerical_features_final,\n",
    "    'target_classes': list(target_encoder.classes_),\n",
    "    'feature_count': X_train_final.shape[1]\n",
    "}\n",
    "\n",
    "feature_info_path = MODELS_DIR / \"feature_info.joblib\"\n",
    "joblib.dump(feature_info, feature_info_path)\n",
    "print(f\"✅ Feature info saved: {feature_info_path}\")\n",
    "\n",
    "# Create preprocessing summary\n",
    "preprocessing_summary = f\"\"\"\n",
    "# Data Preprocessing Summary Report\n",
    "\n",
    "## Dataset Information\n",
    "- **Input dataset**: {data_path}\n",
    "- **Processed data directory**: {processed_data_dir}\n",
    "- **Training samples**: {len(X_train_final)}\n",
    "- **Validation samples**: {len(X_val)}\n",
    "- **Test samples**: {len(X_test)}\n",
    "- **Total features**: {X_train_final.shape[1]}\n",
    "- **Target classes**: {len(target_encoder.classes_)}\n",
    "\n",
    "## Preprocessing Steps Completed\n",
    "1. ✅ Feature selection and preparation\n",
    "2. ✅ Categorical encoding (Label Encoding / One-Hot Encoding)\n",
    "3. ✅ Target variable encoding\n",
    "4. ✅ Feature scaling ({scaling_method})\n",
    "5. ✅ Correlation analysis and multicollinearity check\n",
    "6. ✅ Data splitting (train/val/test)\n",
    "7. ✅ Class balancing {'(SMOTE applied)' if imbalance_ratio > balance_threshold else '(not needed)'}\n",
    "8. ✅ Feature importance analysis\n",
    "9. ✅ Data validation and quality checks\n",
    "10. ✅ Export preprocessed datasets\n",
    "\n",
    "## Key Configuration\n",
    "- **Scaling method**: {scaling_method}\n",
    "- **Test size**: {test_size * 100}%\n",
    "- **Validation size**: {val_size * 100}% of training data\n",
    "- **Class balancing**: {'Applied (SMOTE)' if imbalance_ratio > balance_threshold else 'Not applied'}\n",
    "- **Random state**: {random_state}\n",
    "\n",
    "## Target Distribution (Training)\n",
    "{dict(zip(*np.unique(y_train_final, return_counts=True)))}\n",
    "\n",
    "## Next Steps\n",
    "Proceed to the **Feature Engineering Notebook** for advanced feature creation and selection.\n",
    "\"\"\"\n",
    "\n",
    "print(preprocessing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 Notebook Summary\n",
    "\n",
    "This notebook has successfully completed the data preprocessing process for the credit score classification dataset. The key accomplishments include:\n",
    "\n",
    "### ✅ **Completed Tasks:**\n",
    "1. **Data Loading**: Successfully loaded the cleaned dataset\n",
    "2. **Feature Selection**: Identified and prepared relevant features for modeling\n",
    "3. **Categorical Encoding**: Applied appropriate encoding techniques for categorical variables\n",
    "4. **Target Encoding**: Encoded target variable for machine learning compatibility\n",
    "5. **Feature Scaling**: Applied scaling to ensure feature consistency\n",
    "6. **Correlation Analysis**: Analyzed feature relationships and multicollinearity\n",
    "7. **Data Splitting**: Created train/validation/test splits with stratification\n",
    "8. **Class Balancing**: Applied SMOTE if needed to handle class imbalance\n",
    "9. **Feature Importance**: Analyzed feature relevance to target variable\n",
    "10. **Quality Assurance**: Performed comprehensive validation checks\n",
    "11. **Data Export**: Saved all preprocessed datasets and encoders\n",
    "\n",
    "### 🎯 **Key Outcomes:**\n",
    "- Clean, encoded, and scaled datasets ready for model training\n",
    "- Balanced class distribution (if imbalance was detected)\n",
    "- Comprehensive feature analysis and importance ranking\n",
    "- Saved preprocessors for consistent future data processing\n",
    "\n",
    "### ➡️ **Next Step:**\n",
    "Proceed to the **Feature Engineering Notebook** to create advanced features and perform feature selection.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.8.0"\n",
  }\n},\n"nbformat": 4,\n"nbformat_minor": 4\n}